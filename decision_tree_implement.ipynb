{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from anytree import Node,RenderTree\n",
    "from anytree.exporter import DotExporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to calculate the entropy \n",
    "def entropy(count):\n",
    "    tot=count.sum()\n",
    "    prob=count/tot\n",
    "    ent=(prob*-np.log2(prob)).sum()\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function is used to get the mid value of each feature data on the basis of mid value, split the data into two array\n",
    "and calculate the information gain and gain ratio.\n",
    "\"\"\"\"\n",
    "def split_data(x,train_data,f,infn):\n",
    "    D=len(train_data)\n",
    "    list_inf=[]\n",
    "    for i in range(0,len(x)-1):\n",
    "        x1=(x[i]+x[i+1])/2\n",
    "        data_below=[j for j in train_data if j[f]<x1]   #split the data into one list which is smaller than mid value\n",
    "        data_above=[j for j in train_data if j[f]>=x1]  #split the data into 2nd list  which is greater thanand equal to mid value\n",
    "        data_below1=np.array(data_below)   \n",
    "        data_above1=np.array(data_above)\n",
    "        u,y1_count=np.unique(data_below1[:,-1],return_counts=True)\n",
    "        v,y2_count=np.unique(data_above1[:,-1],return_counts=True)\n",
    "        tot1=y1_count.sum()\n",
    "        tot2=y2_count.sum()\n",
    "        infd1=entropy(y1_count)           # Calculate entropy of left side array  \n",
    "        infd2=entropy(y2_count)           # Calculate entropy of right side array\n",
    "        infF=(abs(tot1)/abs(D))*infd1+(abs(tot2)/abs(D))*infd2 \n",
    "        inform_gain=infn-infF             #Calculate Information gain    \n",
    "        split_info=-((abs(tot1)/abs(D))*np.log2(abs(tot1)/abs(D))+(abs(tot2)/abs(D))*np.log2(abs(tot2)/abs(D)))    #Calculate split info   \n",
    "        gain=inform_gain/split_info       # Calculate gain ratio\n",
    "        list_inf.append([gain,f,x1])                            \n",
    "    return list_inf     \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function is used to calculate the parent node entropy and select the maximum gain ratio of each feature\"\"\"\n",
    "def gain_func(train_data,f):    #train_data is parent data and f is a single feature ont the basis of gain ratio calculate \n",
    "    u,y_count=np.unique(train_data[:,-1],return_counts=True)  \n",
    "    infn=entropy(y_count)\n",
    "    x1=np.unique(train_data[:,f])    #get the unique value of  each feature\n",
    "    split=split_data(x1,train_data,f,infn)\n",
    "    gain=[]\n",
    "    try:\n",
    "        gain=max(split)\n",
    "    except:\n",
    "        pass\n",
    "    return gain\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function is used to split the parent node after getting one maximum gain ratio '''\n",
    "\n",
    "def split_node(train_data,f,mid):               #train_data is parent data, f is particular feature and mid is mid_value by which we get maximum gain ratio  \n",
    "    data_below=[j for j in train_data if j[f]<mid]    #split the parent data into left child data \n",
    "    data_above=[j for j in train_data if j[f]>=mid]   #split the parent data into right child data \n",
    "    data_below1=np.array(data_below)\n",
    "    data_above1=np.array(data_above)\n",
    "    return data_below1,data_above1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of  0 =  37\n",
      "Count of  1 =  34\n",
      "Count of  2 =  41\n",
      "Current Entropy is = 1.5807197138422102\n",
      "Splitting on feature petal width (cm) with gain ratio 0.9999999999999999\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 37\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of  1 =  34\n",
      "Count of  2 =  41\n",
      "Current Entropy is = 0.993707106604508\n",
      "Splitting on feature petal width (cm) with gain ratio 0.6610420198933152\n",
      "\n",
      "Level 2\n",
      "Count of  1 =  33\n",
      "Count of  2 =  4\n",
      "Current Entropy is = 0.49418293484978865\n",
      "Splitting on feature petal length (cm) with gain ratio 0.6941833044972409\n",
      "\n",
      "Level 3\n",
      "Count of 1 = 32\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of  1 =  1\n",
      "Count of  2 =  4\n",
      "Current Entropy is = 0.7219280948873623\n",
      "Splitting on feature petal width (cm) with gain ratio 0.33155970728682876\n",
      "\n",
      "Level 4\n",
      "Count of 2 = 3\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of  1 =  1\n",
      "Count of  2 =  1\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature petal length (cm) with gain ratio 1.0\n",
      "\n",
      "Level 5\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 5\n",
      "Count of 2 = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of  1 =  1\n",
      "Count of  2 =  37\n",
      "Current Entropy is = 0.17556502585750278\n",
      "Splitting on feature petal length (cm) with gain ratio 0.18573556471891253\n",
      "\n",
      "Level 3\n",
      "Count of  1 =  1\n",
      "Count of  2 =  3\n",
      "Current Entropy is = 0.8112781244591328\n",
      "Splitting on feature sepal width (cm) with gain ratio 1.0\n",
      "\n",
      "Level 4\n",
      "Count of 2 = 3\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 2 = 34\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''This function firstly calculate the total sample if sample is equal to 1 it means it is leaf node and if sample is not equal to 1\n",
    "then it call gain function which returns each feature gain ratio, select maximum gain ratio and call the split node which splits \n",
    "parent data into left child node sample and right child node sample then again call decision tree function for left child node and right \n",
    "child node. In the last draw the graph by using DotExporter built in function'''\n",
    "\n",
    "def decision_tree(train_data,feature,counter=-1,parent=None):\n",
    "    count=np.unique(train_data[:,-1],return_counts=True)\n",
    "    counter+=1           \n",
    "    ent=entropy(count[1])    \n",
    "    c_z=(train_data[:,-1]==0).sum()\n",
    "    c_o=(train_data[:,-1]==1).sum()\n",
    "    c_t=(train_data[:,-1]==2).sum()\n",
    "    show_list=[c_z,c_o,c_t]\n",
    "    if len(count[0])==1:               #check total sample value is 1 or not\n",
    "        parent1=parent      \n",
    "        child=Node('Entropy = '+str(ent)+' Sample = '+str((count[1]).sum())+' Value ='+str(show_list),parent=parent1)   #Add child into parent node in tree\n",
    "        print('Level',counter)         # print Level and each information related to leaf node in output \n",
    "        for i in range(len(count[0])):\n",
    "            print('Count of '+ str(int(count[0][i])) + ' = '+str(count[1][i]))\n",
    "        print('Current Entropy is = '+str(ent))\n",
    "        print('Reached Leaf Node')\n",
    "        print()\n",
    "    else:       \n",
    "        max_gain=0\n",
    "        feat=len(feature)\n",
    "        list_gain=[]\n",
    "        for f in range(feat):           #call gain function for each feature\n",
    "            gain=gain_func(train_data,f)\n",
    "            list_gain.append(gain)\n",
    "        s_gain=max(list_gain)          # get maximum gain ratio\n",
    "        \n",
    "        if parent is None:            #check wheather parent is First or Not\n",
    "            child=Node(str(feature[s_gain[1]])+ '<'+ str(s_gain[2]) + ' Entropy ='+ str(round(ent,3))+' Sample ='+ str((count[1]).sum()) +' Value='+str(show_list))\n",
    "            \n",
    "        else:                         # Add child Node into parent node in tree \n",
    "            parent1=parent\n",
    "            child=Node(str(feature[s_gain[1]])+'<'+str(round(s_gain[2],3))+' Entropy = '+str(round(ent,3))+' Sample ='+str((count[1]).sum())+' Value='+str(show_list),parent=parent1)\n",
    "        print('Level',counter)        # Print all information of parent Node in Output\n",
    "        for i in range(len(count[0])):\n",
    "            print('Count of ',int(count[0][i]) , '= ',str(count[1][i]))\n",
    "        print('Current Entropy is = '+str(ent))\n",
    "        print('Splitting on feature '+ str(feature[s_gain[1]]) +' with gain ratio '+str(s_gain[0]))\n",
    "        print()\n",
    "        final_split=split_node(train_data,s_gain[1],s_gain[2])    # get left child node and right child node sample      \n",
    "        decision_tree(final_split[0],feature,counter,child)       # call decision tree for left Child\n",
    "        decision_tree(final_split[1],feature,counter,child)       # call decision tree for right Child\n",
    "    \n",
    "    '''DotExproter function is used to draw tree where child is pick as first parent node.When we execute the code iris_tree.png\n",
    "    Image will create where we can see tree.'''   \n",
    "    \n",
    "    DotExporter(child,nodeattrfunc=lambda node: \"fixedsize=False, width=3, height=1, shape=rectangle\",edgeattrfunc=lambda parent, child: \"style=bold\").to_picture(\"iris_tree.png\")  \n",
    "    \n",
    "df=datasets.load_iris()\n",
    "X=df.data\n",
    "Y=df.target\n",
    "dat\n",
    "print()\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=1)\n",
    "feature=df.feature_names\n",
    "\n",
    "sh=X_train.shape\n",
    "train_data=np.insert(X_train,sh[1],Y_train,axis=1)          # Insert Y column in X_train\n",
    "decision_tree(train_data,feature)                          # Call the decision tree\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
